{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaa: Solution\n",
    "\n",
    "## 1. RNN's (Recursive Neural Networks)\n",
    "\n",
    "### (a) Gradients\n",
    "\n",
    "* Let $\\delta_{a} \\in \\mathbb{R}^d$ be the error coming from the parent node *above*, and $\\delta_{b} \\in \\mathbb{R}^d$ the error going to the child node *below*, where these errors are through the $h^1$ calculations.\n",
    "\n",
    "\n",
    "* Let $\\delta_3 \\in \\mathbb{R}^5$ be the error at that node from the softmax layer i.e. $$\\delta_3 = \\hat y - y$$\n",
    "    \n",
    "    \n",
    "* $U$: $$\\frac{\\partial J}{\\partial U} = \\delta_3 {h^1}^T$$\n",
    "  \n",
    "  \n",
    "* $b^s$: $$\\frac{\\partial J}{\\partial {b^s}} = \\delta_3$$\n",
    "\n",
    "  \n",
    "* $h^1$: $$\\frac{\\partial J}{\\partial {h^1}} = U^T \\delta_3 + {W^1_{left/right}}^T \\delta_a$$ where $\\delta_a$ is the error vector coming from the parent node above and $W^1_{left/right}$ is the left or right half of $W^1$ depending on whether this is the left or right child of the parent node above\n",
    "  \n",
    "  \n",
    "* $\\delta_b$ the error vector going below from $\\hat y$ and from the node above: \n",
    "$$\\delta_b = \\left( U^T \\delta_3 +  {W^1_{left/right}}^T \\delta_a \\right) \\circ \\mathbb{1}\\{W^1 h^1_b + b^1 > 0\\}$$ where $\\mathbb{1}\\{x\\}$ is the indicator function of $x$ i.e. equals 1 if $x$ is $true$ and zero otherwise and $$h^1_b = \\left[ \\begin{array}{c}  h^1_{left} \\\\ h^1_{right} \\end{array} \\right]$$ is the concatenation of the $h^1$ vectors coming from the children nodes below\n",
    "\n",
    "  \n",
    "* $W^1$: $$\\frac{\\partial J}{\\partial {W^1}} = \\delta_b {h^1_b}^T =  \\delta_b \\left[ \\begin{array}{c}  h^1_{left} \\\\ h^1_{right} \\end{array} \\right]^T  = \\left[ \\delta_b {h^1_{left}}^T  \\quad \\delta_b {h^1_{right}}^T  \\right] = \\left[ \\frac{\\partial J}{\\partial {W^1_{left}}} \\quad \\frac{\\partial J}{\\partial {W^1_{right}}} \\right]$$  where $W^1_{left} \\in \\mathbb{R}^{d \\times d}$ is the left half of the matrix $W^1$ and the same for $W^1_{right}$\n",
    "\n",
    "  \n",
    "* $b^1$: $$\\frac{\\partial J}{\\partial {b^1}} = \\delta_b $$  \n",
    "\n",
    "  \n",
    "* $L_i$: $$\\frac{\\partial J}{\\partial {L_i}} = U^T \\delta_3 + {W^1_{left/right}}^T \\delta_a $$   \n",
    "\n",
    "\n",
    "In the code `rnn.py` there is a slightly different notation, where each node computes the $\\delta$ to be passed to each of its two children by multiplying by the right part of the matrix $W$, so for example when computing $\\delta_b$ the $\\delta_a$ is already premultiplied by the left/right half of $W$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2-Layer Deep RNN's\n",
    "\n",
    "### (a) Gradients\n",
    "\n",
    "* Let $\\delta_3 \\in \\mathbb{R}^5$ be the error at that node from the softmax layer i.e. $$\\delta_3 = \\hat y - y$$\n",
    "    \n",
    "    \n",
    "* $U$: $$\\frac{\\partial J}{\\partial U} = \\delta_3 {h^2}^T$$\n",
    "  \n",
    "  \n",
    "* $b^s$: $$\\frac{\\partial J}{\\partial {b^s}} = \\delta_3$$\n",
    "  \n",
    "  \n",
    "* Let $\\delta_2 \\in \\mathbb{R}^{d_{middle}}$ be the error from the softmax after propagating through $h^2$ i.e. $$\\delta_2 = U^T \\delta_3 \\circ \\mathbf{1}\\{W^2 h^1 + b^2 > 0\\} $$\n",
    "\n",
    "  \n",
    "* $W^2$: $$\\frac{\\partial J}{\\partial {W^2}} = \\delta_2 {h^1}^T $$\n",
    "\n",
    "\n",
    "* $b^2$: $$\\frac{\\partial J}{\\partial {b^2}} = \\delta_2 $$\n",
    "\n",
    "\n",
    "* Let $\\delta_a \\in \\mathbb{R}^d$ be the error arriving from the parent node above (zero for the root node) and defined as \n",
    "$$\\delta_a = {W^1_{left/right}}^T \\delta_b  $$ depending on whether it's going to the left/right child, where $\\delta_b$ is the total error before $h^1$ defined as \n",
    "$$\\delta_b = \\left( \\delta_2 + \\delta_a \\right) \\circ \\mathbf{1}\\{W^1 h^1_b + b^1 > 0\\}$$ where this $\\delta_a$ is the error received for this node from its parent\n",
    "\n",
    "\n",
    "* $W^1$: $$\\frac{\\partial J}{\\partial {W^1}} = \\delta_b {h^1_b}^T $$ where $h^1_b$ is defined as \n",
    "$$h^1_b = \\left[ \\begin{array}{c}  h^1_{left} \\\\ h^1_{right} \\end{array} \\right]$$\n",
    "\n",
    "\n",
    "* $b^1$: $$\\frac{\\partial J}{\\partial {b^1}} = \\delta_b $$  \n",
    "\n",
    "  \n",
    "* $L_i$: $$\\frac{\\partial J}{\\partial {L_i}} = {W^2}^T \\delta_2 + \\delta_a $$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Recursive Neural Tensor Networks (RNTN)\n",
    "\n",
    "## Definition\n",
    "* Let $h \\in \\mathbb{R}^d$ be the activation of a node given its children vectors $h_1 \\in \\mathbb{R}^d$ and $h_2 \\in \\mathbb{R}^d$ that are concatenated in the vector $h_b \\in  \\mathbb{R}^{2d}$:\n",
    "\n",
    "$$h_b = \\left[ \\begin{array}{c} h_1 \\\\ h_2 \\end{array} \\right] $$\n",
    "\n",
    "the $i$th element of $h$ is defined as:\n",
    "\n",
    "$$h[i] = f\\left( h_b^T  V[i] h_b + W[i] h_b + b[i]  \\right)$$\n",
    "\n",
    "where $V[i] \\in \\mathbb{R}^{2d\\times2d}$ and $W \\in \\mathbb{R}^{d\\times2d}$ and $b \\in \\mathbb{R}^d$ and $W[i]$ is the $i$th row of $W$ and $b[i]$ is the $i$th element of $b$\n",
    "\n",
    "\n",
    "* The hidden activation is passed to a softmax layer to produce $\\hat y$ as $$\\hat y = softmax\\left( W_s h + b_s \\right)$$\n",
    "\n",
    "\n",
    "## Gradients\n",
    "\n",
    "* Let $\\delta_s \\in \\mathbb{R}^5$ be the error at that node from the softmax layer i.e. $$\\delta_s = \\hat y - y$$\n",
    "    \n",
    "    \n",
    "* $W_s$: $$\\frac{\\partial J}{\\partial W_s} = \\delta_s {h}^T$$\n",
    "  \n",
    "  \n",
    "* $b^s$: $$\\frac{\\partial J}{\\partial {b^s}} = \\delta_s$$\n",
    "\n",
    "\n",
    "* Let $\\delta_a \\in \\mathbb{R}^d$ be the error arriving from the parent node above (zero for the root node) and defined as \n",
    "\n",
    "$$\\delta_a = {W_{left/right}}^T \\delta_b + \\sum_i \\delta_b[i] \\left(V[i]_{up/down} + {V[i]_{up/down}}^T \\right) h_{1/2}$$ \n",
    "\n",
    "depending on whether it's going to the left/right child, where $\\delta_b$ is the total error before $h$ defined as\n",
    "\n",
    "$$\\delta_b = \\left( W_s^T \\delta_s + \\delta_a \\right) \\circ f'(h_b^T V[1:d] h_b + W h_b + b)$$\n",
    "\n",
    "where this $\\delta_a$ is the error received for this node from its parent.\n",
    "\n",
    "\n",
    "* $W$: $$\\frac{\\partial J}{\\partial {W}} = \\delta_b {h_b}^T $$ \n",
    "\n",
    "  \n",
    "* $V[i]$: $$\\frac{\\partial J}{\\partial {V[i]}} = \\delta_b[i] h_b {h_b}^T $$ \n",
    "\n",
    "\n",
    "* $b$: $$\\frac{\\partial J}{\\partial {b}} = \\delta_b $$  \n",
    "\n",
    "  \n",
    "* $L_i$: $$\\frac{\\partial J}{\\partial {L_i}} = {W_s}^T \\delta_s + \\delta_a $$   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
