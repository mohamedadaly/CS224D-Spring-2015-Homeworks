{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaa: Solution\n",
    "\n",
    "## 1. RNN's (Recursive Neural Networks)\n",
    "\n",
    "### (a) Gradients\n",
    "\n",
    "* Let $\\delta_{a} \\in \\mathbb{R}^d$ be the error coming from the parent node *above*, and $\\delta_{b} \\in \\mathbb{R}^d$ the error going to the child node *below*, where these errors are through the $h^1$ calculations.\n",
    "\n",
    "\n",
    "* Let $\\delta_3 \\in \\mathbb{R}^5$ be the error at that node from the softmax layer i.e. $$\\delta_3 = \\hat y - y$$\n",
    "    \n",
    "    \n",
    "* $U$: $$\\frac{\\partial J}{\\partial U} = \\delta_3 {h^1}^T$$\n",
    "  \n",
    "  \n",
    "* $b^s$: $$\\frac{\\partial J}{\\partial {b^s}} = \\delta_3$$\n",
    "\n",
    "  \n",
    "* $h^1$: $$\\frac{\\partial J}{\\partial {h^1}} = U^T \\delta_3 + {W^1_{left/right}}^T \\delta_a$$ where $\\delta_a$ is the error vector coming from the parent node above and $W^1_{left/right}$ is the left or right half of $W^1$ depending on whether this is the left or right child of the parent node above\n",
    "  \n",
    "  \n",
    "* $\\delta_b$ the error vector going below from $\\hat y$ and from the node above: \n",
    "$$\\delta_b = \\left( U^T \\delta_3 +  {W^1_{left/right}}^T \\delta_a \\right) \\circ \\mathbb{1}\\{W^1 h^1_b + b^1 > 0\\}$$ where $\\mathbb{1}\\{x\\}$ is the indicator function of $x$ i.e. equals 1 if $x$ is $true$ and zero otherwise and $$h^1_b = \\left[ \\begin{array}{c}  h^1_{left} \\\\ h^1_{right} \\end{array} \\right]$$ is the concatenation of the $h^1$ vectors coming from the children nodes below\n",
    "\n",
    "  \n",
    "* $W^1$: $$\\frac{\\partial J}{\\partial {W^1}} = \\delta_b {h^1_b}^T =  \\delta_b \\left[ \\begin{array}{c}  h^1_{left} \\\\ h^1_{right} \\end{array} \\right]^T  = \\left[ \\delta_b {h^1_{left}}^T  \\quad \\delta_b {h^1_{right}}^T  \\right] = \\left[ \\frac{\\partial J}{\\partial {W^1_{left}}} \\quad \\frac{\\partial J}{\\partial {W^1_{right}}} \\right]$$  where $W^1_{left} \\in \\mathbb{R}^{d \\times d}$ is the left half of the matrix $W^1$ and the same for $W^1_{right}$\n",
    "\n",
    "  \n",
    "* $b^1$: $$\\frac{\\partial J}{\\partial {b^1}} = \\delta_b $$  \n",
    "\n",
    "  \n",
    "* $L_i$: $$\\frac{\\partial J}{\\partial {L_i}} = U^T \\delta_3 + {W^1_{left/right}}^T \\delta_a $$   \n",
    "\n",
    "\n",
    "In the code `rnn.py` there is a slightly different notation, where each node computes the $\\delta$ to be passed to each of its two children by multiplying by the right part of the matrix $W$, so for example when computing $\\delta_b$ the $\\delta_a$ is already premultiplied by the left/right half of $W$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2-Layer Deep RNN's\n",
    "\n",
    "### (a) Gradients\n",
    "\n",
    "* Let $\\delta_3 \\in \\mathbb{R}^5$ be the error at that node from the softmax layer i.e. $$\\delta_3 = \\hat y - y$$\n",
    "    \n",
    "    \n",
    "* $U$: $$\\frac{\\partial J}{\\partial U} = \\delta_3 {h^2}^T$$\n",
    "  \n",
    "  \n",
    "* $b^s$: $$\\frac{\\partial J}{\\partial {b^s}} = \\delta_3$$\n",
    "  \n",
    "  \n",
    "* Let $\\delta_2 \\in \\mathbb{R}^{d_{middle}}$ be the error from the softmax after propagating through $h^2$ i.e. $$\\delta_2 = U^T \\delta_3 \\circ \\mathbf{1}\\{W^2 h^1 + b^2 > 0\\} $$\n",
    "\n",
    "  \n",
    "* $W^2$: $$\\frac{\\partial J}{\\partial {W^2}} = \\delta_2 {h^1}^T $$\n",
    "\n",
    "\n",
    "* $b^2$: $$\\frac{\\partial J}{\\partial {b^2}} = \\delta_2 $$\n",
    "\n",
    "\n",
    "* Let $\\delta_a \\in \\mathbb{R}^d$ be the error arriving from the parent node above (zero for the root node) and defined as \n",
    "$$\\delta_a = {W^1_{left/right}}^T \\delta_b  $$ depending on whether it's going to the left/right child, where $\\delta_b$ is the total error before $h^1$ defined as \n",
    "$$\\delta_b = \\left( \\delta_2 + \\delta_a \\right) \\circ \\mathbf{1}\\{W^1 h^1_b + b^1 > 0\\}$$ where this $\\delta_a$ is the error received for this node from its parent\n",
    "\n",
    "\n",
    "* $W^1$: $$\\frac{\\partial J}{\\partial {W^1}} = \\delta_b {h^1_b}^T $$ where $h^1_b$ is defined as \n",
    "$$h^1_b = \\left[ \\begin{array}{c}  h^1_{left} \\\\ h^1_{right} \\end{array} \\right]$$\n",
    "\n",
    "\n",
    "* $b^1$: $$\\frac{\\partial J}{\\partial {b^1}} = \\delta_b $$  \n",
    "\n",
    "  \n",
    "* $L_i$: $$\\frac{\\partial J}{\\partial {L_i}} = {W^2}^T \\delta_2 + \\delta_a $$   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
